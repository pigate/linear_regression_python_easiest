{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_data = [(i, 2*i + 20) for i in range(400,405)]\n",
    "data = pd.DataFrame(linear_data, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x    y\n",
      "0  400  820\n",
      "1  401  822\n",
      "2  402  824\n",
      "3  403  826\n",
      "4  404  828\n"
     ]
    }
   ],
   "source": [
    "print data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split population and profit into X and y\n",
    "X_df = pd.DataFrame(data.x)\n",
    "y_df = pd.DataFrame(data.y)\n",
    "y_df['y'] = y_df['y']\n",
    "\n",
    "## Length, or number of observations, in our data\n",
    "m = len(y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y\n",
       "0  820\n",
       "1  822\n",
       "2  824\n",
       "3  826\n",
       "4  828"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAHjCAYAAACXcOPPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e079250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(X_df, y_df, 'kx')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add a columns of 1s as intercept to X\n",
    "X_df['intercept'] = 1\n",
    "\n",
    "## Transform to Numpy arrays for easier matrix math and start theta at 0\n",
    "X = np.array(X_df)\n",
    "y = np.array(y_df).flatten()\n",
    "theta = np.array([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost_function(X, y, theta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, theta) computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(y) \n",
    "    \n",
    "    ## Calculate the cost with the given parameters\n",
    "    J = np.sum((X.dot(theta)-y)**2)/2/m\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339492"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_function(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    gradient_descent Performs gradient descent to learn theta\n",
    "    theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by \n",
    "    taking num_iters gradient steps with learning rate alpha\n",
    "    \"\"\"\n",
    "    cost_history = [(0,0)] * iterations\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        hypothesis = X.dot(theta)\n",
    "        loss = hypothesis-y\n",
    "        print \"loss: {0}\\ntheta: {1}\".format(loss, theta)\n",
    "        gradient = X.T.dot(loss)/m \n",
    "        print \"gradient: {0}\".format(gradient)\n",
    "        theta = theta - alpha*gradient\n",
    "        cost = cost_function(X, y, theta)\n",
    "        print \"cost: {0}\".format(cost)\n",
    "        cost_history[iteration] = (iteration, cost)\n",
    "\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [-820 -822 -824 -826 -828]\n",
      "theta: [0 0]\n",
      "gradient: [-331252    -824]\n",
      "cost: 78031157.5609\n",
      "loss: [ 12430.1624  12461.2876  12492.4128  12523.538   12554.6632]\n",
      "theta: [ 33.1252   0.0824]\n",
      "gradient: [ 5022012.196     12492.4128]\n",
      "cost: 17935213773.2\n",
      "loss: [-188451.57468128 -188922.65070088 -189393.72672048 -189864.80274008\n",
      " -190335.87875968]\n",
      "theta: [-469.0760196    -1.16684128]\n",
      "gradient: [-76137220.29367216   -189393.72672048]\n",
      "cost: 4.12235193163e+12\n",
      "loss: [ 2857056.17643828  2864198.82244805  2871341.46845781  2878484.11446758\n",
      "  2885626.76047735]\n",
      "theta: [ 7144.64600977    17.77253139]\n",
      "gradient: [  1.15429356e+09   2.87134147e+06]\n",
      "cost: 9.47509500762e+14\n",
      "loss: [-43314973.18219098 -43423259.89174242 -43531546.60129386 -43639833.3108453\n",
      " -43748120.02039674]\n",
      "theta: [-108284.70955144    -269.36161545]\n",
      "gradient: [ -1.74998983e+10  -4.35315466e+07]\n",
      "cost: 2.17782049889e+17\n",
      "loss: [  6.56685312e+08   6.58327015e+08   6.59968719e+08   6.61610422e+08\n",
      "   6.63252125e+08]\n",
      "theta: [ 1641705.12116248     4083.79304468]\n",
      "gradient: [  2.65310708e+11   6.59968719e+08]\n",
      "cost: 5.00565125897e+19\n",
      "loss: [ -9.95580901e+09  -9.98069838e+09  -1.00055877e+10  -1.00304771e+10\n",
      "  -1.00553665e+10]\n",
      "theta: [-24889365.70317636    -61913.07880536]\n",
      "gradient: [ -4.02229605e+12  -1.00055877e+10]\n",
      "cost: 1.15053304619e+22\n",
      "loss: [  1.50937034e+11   1.51314374e+11   1.51691714e+11   1.52069054e+11\n",
      "   1.52446395e+11]\n",
      "theta: [  3.77340240e+08   9.38645696e+05]\n",
      "gradient: [  6.09808238e+13   1.51691714e+11]\n",
      "cost: 2.64446367094e+24\n",
      "loss: [ -2.28831109e+12  -2.29403183e+12  -2.29975257e+12  -2.30547331e+12\n",
      "  -2.31119406e+12]\n",
      "theta: [ -5.72074214e+09  -1.42305257e+07]\n",
      "gradient: [ -9.24511975e+14  -2.29975257e+12]\n",
      "cost: 6.07821577146e+26\n",
      "loss: [  3.46923979e+13   3.47791283e+13   3.48658588e+13   3.49525893e+13\n",
      "   3.50393197e+13]\n",
      "theta: [  8.67304554e+10   2.15744731e+08]\n",
      "gradient: [  1.40162487e+16   3.48658588e+13]\n",
      "cost: 1.39705859341e+29\n",
      "loss: [ -5.25961037e+14  -5.27275931e+14  -5.28590825e+14  -5.29905720e+14\n",
      "  -5.31220614e+14]\n",
      "theta: [ -1.31489441e+12  -3.27084115e+09]\n",
      "gradient: [ -2.12496142e+17  -5.28590825e+14]\n",
      "cost: 3.21109481269e+31\n",
      "loss: [  7.97393749e+15   7.99387221e+15   8.01380693e+15   8.03374165e+15\n",
      "   8.05367637e+15]\n",
      "theta: [  1.99347197e+13   4.95882414e+10]\n",
      "gradient: [  3.22159025e+18   8.01380693e+15]\n",
      "cost: 7.38059945712e+33\n",
      "loss: [ -1.20890474e+17  -1.21192698e+17  -1.21494923e+17  -1.21797147e+17\n",
      "  -1.22099371e+17]\n",
      "theta: [ -3.02224306e+14  -7.51792451e+11]\n",
      "gradient: [ -4.88415634e+19  -1.21494923e+17]\n",
      "cost: 1.69640734778e+36\n",
      "loss: [  1.83278421e+18   1.83736614e+18   1.84194807e+18   1.84653001e+18\n",
      "   1.85111194e+18]\n",
      "theta: [  4.58193203e+15   1.13976998e+13]\n",
      "gradient: [  7.40472290e+20   1.84194807e+18]\n",
      "cost: 3.89913841864e+38\n",
      "loss: [ -2.77862916e+19  -2.78557569e+19  -2.79252222e+19  -2.79946875e+19\n",
      "  -2.80641528e+19]\n",
      "theta: [ -6.94652969e+16  -1.72797108e+14]\n",
      "gradient: [ -1.12260782e+22  -2.79252222e+19]\n",
      "cost: 8.96204583625e+40\n",
      "loss: [  4.21259631e+20   4.22312773e+20   4.23365916e+20   4.24419058e+20\n",
      "   4.25472201e+20]\n",
      "theta: [  1.05314253e+18   2.61972511e+15]\n",
      "gradient: [  1.70195204e+23   4.23365916e+20]\n",
      "cost: 2.05989777606e+43\n",
      "loss: [ -6.38659088e+21  -6.40255726e+21  -6.41852364e+21  -6.43449001e+21\n",
      "  -6.45045639e+21]\n",
      "theta: [ -1.59663779e+19  -3.97168665e+16]\n",
      "gradient: [ -2.58027843e+24  -6.41852364e+21]\n",
      "cost: 4.73460962524e+45\n",
      "loss: [  9.68251884e+22   9.70672498e+22   9.73093113e+22   9.75513728e+22\n",
      "   9.77934342e+22]\n",
      "theta: [  2.42061466e+20   6.02135497e+17]\n",
      "gradient: [  3.91188273e+25   9.73093113e+22]\n",
      "cost: 1.08823498738e+48\n",
      "loss: [ -1.46793763e+24  -1.47160745e+24  -1.47527728e+24  -1.47894710e+24\n",
      "  -1.48261692e+24]\n",
      "theta: [ -3.66982126e+21  -9.12879563e+18]\n",
      "gradient: [ -5.93068804e+26  -1.47527728e+24]\n",
      "cost: 2.50127356108e+50\n",
      "loss: [  2.22549621e+25   2.23105991e+25   2.23662362e+25   2.24218732e+25\n",
      "   2.24775103e+25]\n",
      "theta: [  5.56370592e+22   1.38398932e+20]\n",
      "gradient: [  8.99133822e+27   2.23662362e+25]\n",
      "cost: 5.74909785102e+52\n",
      "loss: [ -3.37400803e+26  -3.38244300e+26  -3.39087797e+26  -3.39931294e+26\n",
      "  -3.40774790e+26]\n",
      "theta: [ -8.43496763e+23  -2.09822469e+21]\n",
      "gradient: [ -1.36314981e+29  -3.39087797e+26]\n",
      "cost: 1.32141188452e+55\n",
      "loss: [  5.11523236e+27   5.12802036e+27   5.14080836e+27   5.15359636e+27\n",
      "   5.16638437e+27]\n",
      "theta: [  1.27880014e+25   3.18105550e+22]\n",
      "gradient: [  2.06663054e+30   5.14080836e+27]\n",
      "cost: 3.03722325451e+57\n",
      "loss: [ -7.75505032e+28  -7.77443783e+28  -7.79382533e+28  -7.81321284e+28\n",
      "  -7.83260034e+28]\n",
      "theta: [ -1.93875052e+26  -4.82270281e+23]\n",
      "gradient: [ -3.13315656e+31  -7.79382533e+28]\n",
      "cost: 6.98096120203e+59\n",
      "loss: [  1.17571991e+30   1.17865920e+30   1.18159848e+30   1.18453776e+30\n",
      "   1.18747704e+30]\n",
      "theta: [  2.93928151e+27   7.31155505e+24]\n",
      "gradient: [  4.75008466e+32   1.18159848e+30]\n",
      "cost: 1.60455176391e+62\n",
      "loss: [ -1.78247369e+31  -1.78692985e+31  -1.79138600e+31  -1.79584216e+31\n",
      "  -1.80029832e+31]\n",
      "theta: [ -4.45615651e+28  -1.10848293e+26]\n",
      "gradient: [ -7.20146086e+33  -1.79138600e+31]\n",
      "cost: 3.68801127605e+64\n",
      "loss: [  2.70235489e+32   2.70911073e+32   2.71586658e+32   2.72262242e+32\n",
      "   2.72937827e+32]\n",
      "theta: [  6.75584521e+29   1.68053771e+27]\n",
      "gradient: [  1.09179188e+35   2.71586658e+32]\n",
      "cost: 8.47677680345e+66\n",
      "loss: [ -4.09695917e+33  -4.10720151e+33  -4.11744384e+33  -4.12768618e+33\n",
      "  -4.13792851e+33]\n",
      "theta: [ -1.02423342e+31  -2.54781281e+28]\n",
      "gradient: [ -1.65523291e+36  -4.11744384e+33]\n",
      "cost: 1.94836022986e+69\n",
      "loss: [  6.21127689e+34   6.22680499e+34   6.24233309e+34   6.25786118e+34\n",
      "   6.27338928e+34]\n",
      "theta: [  1.55280957e+32   3.86266256e+29]\n",
      "gradient: [  2.50944896e+37   6.24233309e+34]\n",
      "cost: 4.47824411724e+71\n",
      "loss: [ -9.41673056e+35  -9.44027224e+35  -9.46381392e+35  -9.48735560e+35\n",
      "  -9.51089728e+35]\n",
      "theta: [ -2.35416800e+33  -5.85606683e+30]\n",
      "gradient: [ -3.80450028e+38  -9.46381392e+35]\n",
      "cost: 1.02931018947e+74\n",
      "loss: [  1.42764227e+37   1.43121135e+37   1.43478044e+37   1.43834952e+37\n",
      "   1.44191860e+37]\n",
      "theta: [  3.56908348e+34   8.87820724e+31]\n",
      "gradient: [  5.76788874e+39   1.43478044e+37]\n",
      "cost: 2.36583678426e+76\n",
      "loss: [ -2.16440562e+38  -2.16981660e+38  -2.17522758e+38  -2.18063856e+38\n",
      "  -2.18604954e+38]\n",
      "theta: [ -5.41098039e+35  -1.34599836e+33]\n",
      "gradient: [ -8.74452308e+40  -2.17522758e+38]\n",
      "cost: 5.43780072036e+78\n",
      "loss: [  3.28139042e+39   3.28959385e+39   3.29779727e+39   3.30600070e+39\n",
      "   3.31420412e+39]\n",
      "theta: [  8.20342504e+36   2.04062774e+34]\n",
      "gradient: [  1.32573091e+42   3.29779727e+39]\n",
      "cost: 1.24986122758e+81\n",
      "loss: [ -4.97481758e+40  -4.98725454e+40  -4.99969151e+40  -5.01212848e+40\n",
      "  -5.02456544e+40]\n",
      "theta: [ -1.24369666e+38  -3.09373450e+35]\n",
      "gradient: [ -2.00990086e+43  -4.99969151e+40]\n",
      "cost: 2.87276634165e+83\n",
      "loss: [  7.54217168e+41   7.56102700e+41   7.57988231e+41   7.59873762e+41\n",
      "   7.61759293e+41]\n",
      "theta: [  1.88553119e+39   4.69031806e+36]\n",
      "gradient: [  3.04715040e+44   7.57988231e+41]\n",
      "cost: 6.60296221023e+85\n",
      "loss: [ -1.14344602e+43  -1.14630462e+43  -1.14916322e+43  -1.15202181e+43\n",
      "  -1.15488041e+43]\n",
      "theta: [ -2.85859728e+40  -7.11085050e+37]\n",
      "gradient: [ -4.61969330e+45  -1.14916322e+43]\n",
      "cost: 1.517669896e+88\n",
      "loss: [  1.73354421e+44   1.73787804e+44   1.74221188e+44   1.74654571e+44\n",
      "   1.75087955e+44]\n",
      "theta: [  4.33383358e+41   1.07805471e+39]\n",
      "gradient: [  7.00377843e+46   1.74221188e+44]\n",
      "cost: 3.48831606163e+90\n",
      "loss: [ -2.62817437e+45  -2.63474477e+45  -2.64131516e+45  -2.64788556e+45\n",
      "  -2.65445595e+45]\n",
      "theta: [ -6.57039507e+42  -1.63440641e+40]\n",
      "gradient: [ -1.06182184e+48  -2.64131516e+45]\n",
      "cost: 8.01778369452e+92\n",
      "loss: [  3.98449632e+46   3.99445750e+46   4.00441868e+46   4.01437986e+46\n",
      "   4.02434103e+46]\n",
      "theta: [  9.96117885e+43   2.47787452e+41]\n",
      "gradient: [  1.60979623e+49   4.00441868e+46]\n",
      "cost: 1.84286212133e+95\n",
      "loss: [ -6.04077533e+47  -6.05587718e+47  -6.07097902e+47  -6.08608087e+47\n",
      "  -6.10118271e+47]\n",
      "theta: [ -1.51018444e+45  -3.75663122e+42]\n",
      "gradient: [ -2.44056377e+50  -6.07097902e+47]\n",
      "cost: 4.23576006492e+97\n",
      "loss: [  9.15823826e+48   9.18113371e+48   9.20402917e+48   9.22692462e+48\n",
      "   9.24982007e+48]\n",
      "theta: [  2.28954533e+46   5.69531590e+43]\n",
      "gradient: [  3.70006552e+51   9.20402917e+48]\n",
      "cost: 9.73576000065e+99\n",
      "loss: [ -1.38845303e+50  -1.39192414e+50  -1.39539525e+50  -1.39886636e+50\n",
      "  -1.40233747e+50]\n",
      "theta: [ -3.47111098e+47  -8.63449758e+44]\n",
      "gradient: [ -5.60955833e+52  -1.39539525e+50]\n",
      "cost: 2.23773351978e+102\n",
      "loss: [  2.10499198e+51   2.11025443e+51   2.11551688e+51   2.12077932e+51\n",
      "   2.12604177e+51]\n",
      "theta: [  5.26244723e+48   1.30905027e+46]\n",
      "gradient: [  8.50448309e+53   2.11551688e+51]\n",
      "cost: 5.14335943492e+104\n",
      "loss: [ -3.19131519e+52  -3.19929343e+52  -3.20727167e+52  -3.21524991e+52\n",
      "  -3.22322815e+52]\n",
      "theta: [ -7.97823837e+49  -1.98461185e+47]\n",
      "gradient: [ -1.28933917e+55  -3.20727167e+52]\n",
      "cost: 1.18218483314e+107\n",
      "loss: [  4.83825722e+53   4.85035279e+53   4.86244836e+53   4.87454393e+53\n",
      "   4.88663950e+53]\n",
      "theta: [  1.20955678e+51   3.00881049e+48]\n",
      "gradient: [  1.95472843e+56   4.86244836e+53]\n",
      "cost: 2.71721429812e+109\n",
      "loss: [ -7.33513663e+54  -7.35347436e+54  -7.37181209e+54  -7.39014981e+54\n",
      "  -7.40848754e+54]\n",
      "theta: [ -1.83377275e+52  -4.56156731e+49]\n",
      "gradient: [ -2.96350513e+57  -7.37181209e+54]\n",
      "cost: 6.2454307778e+111\n",
      "loss: [  1.11205806e+56   1.11483819e+56   1.11761831e+56   1.12039844e+56\n",
      "   1.12317857e+56]\n",
      "theta: [  2.78012786e+53   6.91565535e+50]\n",
      "gradient: [  4.49288123e+58   1.11761831e+56]\n",
      "cost: 1.43549243161e+114\n",
      "loss: [ -1.68595786e+57  -1.69017273e+57  -1.69438760e+57  -1.69860247e+57\n",
      "  -1.70281734e+57]\n",
      "theta: [ -4.21486844e+54  -1.04846176e+52]\n",
      "gradient: [ -6.81152244e+59  -1.69438760e+57]\n",
      "cost: 3.29943376932e+116\n",
      "loss: [  2.55603013e+58   2.56242017e+58   2.56881021e+58   2.57520024e+58\n",
      "   2.58159028e+58]\n",
      "theta: [  6.39003560e+55   1.58954142e+53]\n",
      "gradient: [  1.03267448e+61   2.56881021e+58]\n",
      "cost: 7.5836437437e+118\n",
      "loss: [ -3.87512061e+59  -3.88480835e+59  -3.89449609e+59  -3.90418383e+59\n",
      "  -3.91387157e+59]\n",
      "theta: [ -9.68774127e+56  -2.40985606e+54]\n",
      "gradient: [ -1.56560680e+62  -3.89449609e+59]\n",
      "cost: 1.74307643227e+121\n",
      "loss: [  5.87495410e+60   5.88964139e+60   5.90432868e+60   5.91901598e+60\n",
      "   5.93370327e+60]\n",
      "theta: [  1.46872939e+58   3.65351048e+55]\n",
      "gradient: [  2.37356951e+63   5.90432868e+60]\n",
      "cost: 4.00640582735e+123\n",
      "loss: [ -8.90684166e+61  -8.92910862e+61  -8.95137559e+61  -8.97364255e+61\n",
      "  -8.99590952e+61]\n",
      "theta: [ -2.22669657e+59  -5.53897764e+56]\n",
      "gradient: [ -3.59849752e+64  -8.95137559e+61]\n",
      "cost: 9.20859656884e+125\n",
      "loss: [  1.35033954e+63   1.35371537e+63   1.35709120e+63   1.36046703e+63\n",
      "   1.36384285e+63]\n",
      "theta: [  3.37582786e+60   8.39747782e+57]\n",
      "gradient: [  5.45557413e+65   1.35709120e+63]\n",
      "cost: 2.11656667901e+128\n",
      "loss: [ -2.04720927e+64  -2.05232726e+64  -2.05744525e+64  -2.06256324e+64\n",
      "  -2.06768124e+64]\n",
      "theta: [ -5.11799135e+61  -1.27311642e+59]\n",
      "gradient: [ -8.27103228e+66  -2.05744525e+64]\n",
      "cost: 4.86486129913e+130\n",
      "loss: [  3.10371256e+65   3.11147179e+65   3.11923102e+65   3.12699026e+65\n",
      "   3.13474949e+65]\n",
      "theta: [  7.75923314e+62   1.93013361e+60]\n",
      "gradient: [  1.25394639e+68   3.11923102e+65]\n",
      "cost: 1.11817292101e+133\n",
      "loss: [ -4.70544550e+66  -4.71720904e+66  -4.72897258e+66  -4.74073612e+66\n",
      "  -4.75249966e+66]\n",
      "theta: [ -1.17635406e+64  -2.92621766e+61]\n",
      "gradient: [ -1.90107050e+69  -4.72897258e+66]\n",
      "cost: 2.57008495082e+135\n",
      "loss: [  7.13378476e+67   7.15161911e+67   7.16945346e+67   7.18728781e+67\n",
      "   7.20512216e+67]\n",
      "theta: [  1.78343510e+65   4.43635081e+62]\n",
      "gradient: [  2.88215596e+70   7.16945346e+67]\n",
      "cost: 5.90725864519e+137\n",
      "loss: [ -1.08153171e+69  -1.08423552e+69  -1.08693933e+69  -1.08964314e+69\n",
      "  -1.09234696e+69]\n",
      "theta: [ -2.70381245e+66  -6.72581838e+63]\n",
      "gradient: [ -4.36955018e+71  -1.08693933e+69]\n",
      "cost: 1.35776464082e+140\n",
      "loss: [  1.63967777e+70   1.64377694e+70   1.64787611e+70   1.65197528e+70\n",
      "   1.65607445e+70]\n",
      "theta: [  4.09916894e+67   1.01968115e+65]\n",
      "gradient: [  6.62454395e+72   1.64787611e+70]\n",
      "cost: 3.12077891045e+142\n",
      "loss: [ -2.48586628e+71  -2.49208091e+71  -2.49829553e+71  -2.50451016e+71\n",
      "  -2.51072479e+71]\n",
      "theta: [ -6.21462705e+68  -1.54590800e+66]\n",
      "gradient: [ -1.00432723e+74  -2.49829553e+71]\n",
      "cost: 7.17301122386e+144\n",
      "loss: [  3.76874729e+72   3.77816910e+72   3.78759091e+72   3.79701272e+72\n",
      "   3.80643453e+72]\n",
      "theta: [  9.42180964e+69   2.34370473e+67]\n",
      "gradient: [  1.52263039e+75   3.78759091e+72]\n",
      "cost: 1.64869385157e+147\n",
      "loss: [ -5.71368471e+73  -5.72796883e+73  -5.74225295e+73  -5.75653707e+73\n",
      "  -5.77082120e+73]\n",
      "theta: [ -1.42841229e+71  -3.55322044e+68]\n",
      "gradient: [ -2.30841425e+76  -5.74225295e+73]\n",
      "cost: 3.78947046277e+149\n",
      "loss: [  8.66234597e+74   8.68400170e+74   8.70565743e+74   8.72731316e+74\n",
      "   8.74896889e+74]\n",
      "theta: [  2.16557303e+72   5.38693091e+69]\n",
      "gradient: [  3.49971760e+77   8.70565743e+74]\n",
      "cost: 8.70997752224e+151\n",
      "loss: [ -1.31327229e+76  -1.31655545e+76  -1.31983861e+76  -1.32312177e+76\n",
      "  -1.32640493e+76]\n",
      "theta: [ -3.28316030e+73  -8.16696434e+70]\n",
      "gradient: [ -5.30581686e+78  -1.31983861e+76]\n",
      "cost: 2.00196067454e+154\n",
      "loss: [  1.99101271e+77   1.99599021e+77   2.00096772e+77   2.00594522e+77\n",
      "   2.01092272e+77]\n",
      "theta: [  4.97750083e+74   1.23816896e+72]\n",
      "gradient: [  8.04398977e+79   2.00096772e+77]\n",
      "cost: 4.60144303722e+156\n",
      "loss: [ -3.01851464e+78  -3.02606088e+78  -3.03360712e+78  -3.04115336e+78\n",
      "  -3.04869960e+78]\n",
      "theta: [ -7.54623968e+75  -1.87715082e+73]\n",
      "gradient: [ -1.21952516e+81  -3.03360712e+78]\n",
      "cost: 1.05762707e+159\n",
      "loss: [  4.57627950e+79   4.58772012e+79   4.59916075e+79   4.61060138e+79\n",
      "   4.62204201e+79]\n",
      "theta: [  1.14406276e+77   2.84589204e+74]\n",
      "gradient: [  1.84888550e+82   4.59916075e+79]\n",
      "cost: 2.43092223495e+161\n",
      "loss: [ -6.93796006e+80  -6.95530485e+80  -6.97264964e+80  -6.98999443e+80\n",
      "  -7.00733923e+80]\n",
      "theta: [ -1.73447923e+78  -4.31457155e+75]\n",
      "gradient: [ -2.80303985e+83  -6.97264964e+80]\n",
      "cost: 5.58739756195e+163\n",
      "loss: [  1.05184331e+82   1.05447290e+82   1.05710249e+82   1.05973209e+82\n",
      "   1.06236168e+82]\n",
      "theta: [  2.62959192e+79   6.54119249e+76]\n",
      "gradient: [  4.24960462e+84   1.05710249e+82]\n",
      "cost: 1.28424558657e+166\n",
      "loss: [ -1.59466809e+83  -1.59865473e+83  -1.60264138e+83  -1.60662802e+83\n",
      "  -1.61061467e+83]\n",
      "theta: [ -3.98664543e+80  -9.91690569e+77]\n",
      "gradient: [ -6.44269807e+85  -1.60264138e+83]\n",
      "cost: 2.95179769889e+168\n",
      "loss: [  2.41762845e+84   2.42367248e+84   2.42971651e+84   2.43576055e+84\n",
      "   2.44180458e+84]\n",
      "theta: [  6.04403353e+81   1.50347232e+79]\n",
      "gradient: [  9.76758127e+86   2.42971651e+84]\n",
      "cost: 6.78461327517e+170\n",
      "loss: [ -3.66529396e+85  -3.67445714e+85  -3.68362031e+85  -3.69278349e+85\n",
      "  -3.70194667e+85]\n",
      "theta: [ -9.16317791e+82  -2.27936928e+80]\n",
      "gradient: [ -1.48083369e+88  -3.68362031e+85]\n",
      "cost: 1.55942181644e+173\n",
      "loss: [  5.55684221e+86   5.57073423e+86   5.58462625e+86   5.59851827e+86\n",
      "   5.61241029e+86]\n",
      "theta: [  1.38920191e+84   3.45568339e+81]\n",
      "gradient: [  2.24504754e+89   5.58462625e+86]\n",
      "cost: 3.5842815249e+175\n",
      "loss: [ -8.42456177e+87  -8.44562304e+87  -8.46668432e+87  -8.48774559e+87\n",
      "  -8.50880687e+87]\n",
      "theta: [ -2.10612735e+85  -5.23905791e+82]\n",
      "gradient: [ -3.40364922e+90  -8.46668432e+87]\n",
      "cost: 8.23835726441e+177\n",
      "loss: [  1.27722254e+89   1.28041557e+89   1.28360861e+89   1.28680165e+89\n",
      "   1.28999468e+89]\n",
      "theta: [  3.19303648e+86   7.94277853e+83]\n",
      "gradient: [  5.16017047e+91   1.28360861e+89]\n",
      "cost: 1.89356025593e+180\n",
      "loss: [ -1.93635877e+90  -1.94119964e+90  -1.94604050e+90  -1.95088137e+90\n",
      "  -1.95572224e+90]\n",
      "theta: [ -4.84086682e+87  -1.20418082e+85]\n",
      "gradient: [ -7.82317964e+92  -1.94604050e+90]\n",
      "cost: 4.35228811735e+182\n",
      "loss: [  2.93565544e+91   2.94299453e+91   2.95033363e+91   2.95767272e+91\n",
      "   2.96501181e+91]\n",
      "theta: [  7.33909296e+88   1.82562242e+86]\n",
      "gradient: [  1.18604880e+94   2.95033363e+91]\n",
      "cost: 1.00035960288e+185\n",
      "loss: [ -4.45065914e+92  -4.46178572e+92  -4.47291230e+92  -4.48403888e+92\n",
      "  -4.49516546e+92]\n",
      "theta: [ -1.11265787e+90  -2.76777138e+87]\n",
      "gradient: [ -1.79813300e+95  -4.47291230e+92]\n",
      "cost: 2.29929478031e+187\n",
      "loss: [  6.74751081e+93   6.76437948e+93   6.78124815e+93   6.79811682e+93\n",
      "   6.81498550e+93]\n",
      "theta: [  1.68686721e+91   4.19613516e+88]\n",
      "gradient: [  2.72609549e+96   6.78124815e+93]\n",
      "cost: 5.28485603735e+189\n",
      "loss: [ -1.02296987e+95  -1.02552728e+95  -1.02808469e+95  -1.03064210e+95\n",
      "  -1.03319951e+95]\n",
      "theta: [ -2.55740877e+92  -6.36163464e+89]\n",
      "gradient: [ -4.13295160e+97  -1.02808469e+95]\n",
      "cost: 1.21470737787e+192\n",
      "loss: [  1.55089393e+96   1.55477114e+96   1.55864835e+96   1.56252556e+96\n",
      "   1.56640278e+96]\n",
      "theta: [  3.87721072e+93   9.64468342e+90]\n",
      "gradient: [  6.26584393e+98   1.55864835e+96]\n",
      "cost: 2.79196633443e+194\n",
      "loss: [ -2.35126376e+97  -2.35714189e+97  -2.36302001e+97  -2.36889813e+97\n",
      "  -2.37477625e+97]\n",
      "theta: [ -5.87812285e+94  -1.46220152e+92]\n",
      "gradient: [ -9.49945800e+99  -2.36302001e+97]\n",
      "cost: 6.41724595948e+196\n",
      "loss: [  3.56468045e+98   3.57359210e+98   3.58250375e+98   3.59141539e+98\n",
      "   3.60032704e+98]\n",
      "theta: [  8.91164571e+95   2.21679986e+93]\n",
      "gradient: [  1.44018433e+101   3.58250375e+098]\n",
      "cost: 1.47498360552e+199\n",
      "loss: [ -5.40430510e+99  -5.41781577e+99  -5.43132645e+99  -5.44483713e+99\n",
      "  -5.45834781e+99]\n",
      "theta: [ -1.35106787e+97  -3.36082376e+94]\n",
      "gradient: [ -2.18342026e+102  -5.43132645e+099]\n",
      "cost: 3.39020297847e+201\n",
      "loss: [  8.19330482e+100   8.21378796e+100   8.23427109e+100   8.25475423e+100\n",
      "   8.27523736e+100]\n",
      "theta: [  2.04831347e+98   5.09524408e+95]\n",
      "gradient: [  3.31021795e+103   8.23427109e+100]\n",
      "cost: 7.79227388852e+203\n",
      "loss: [ -1.24216236e+102  -1.24526775e+102  -1.24837314e+102  -1.25147852e+102\n",
      "  -1.25458391e+102]\n",
      "theta: [ -3.10538660e+99  -7.72474669e+96]\n",
      "gradient: [ -5.01852212e+104  -1.24837314e+102]\n",
      "cost: 1.79102940854e+206\n",
      "loss: [  1.88320510e+103   1.88791308e+103   1.89262106e+103   1.89732905e+103\n",
      "   1.90203703e+103]\n",
      "theta: [  4.70798346e+100   1.17112567e+098]\n",
      "gradient: [  7.60843083e+105   1.89262106e+103]\n",
      "cost: 4.11662422054e+208\n",
      "loss: [ -2.85507075e+104  -2.86220838e+104  -2.86934601e+104  -2.87648365e+104\n",
      "  -2.88362128e+104]\n",
      "theta: [ -7.13763249e+101  -1.77550850e+099]\n",
      "gradient: [ -1.15349137e+107  -2.86934601e+104]\n",
      "cost: 9.46193004555e+210\n",
      "loss: [  4.32848711e+105   4.33930826e+105   4.35012941e+105   4.36095056e+105\n",
      "   4.37177171e+105]\n",
      "theta: [  1.08211505e+103   2.69179516e+100]\n",
      "gradient: [  1.74877367e+108   4.35012941e+105]\n",
      "cost: 2.17479457416e+213\n",
      "loss: [ -6.56228945e+106  -6.57869507e+106  -6.59510070e+106  -6.61150632e+106\n",
      "  -6.62791194e+106]\n",
      "theta: [ -1.64056216e+104  -4.08094989e+101]\n",
      "gradient: [ -2.65126329e+109  -6.59510070e+106]\n",
      "cost: 4.99869626707e+215\n",
      "loss: [  9.94889017e+107   9.97376224e+107   9.99863431e+107   1.00235064e+108\n",
      "   1.00483785e+108]\n",
      "theta: [  2.48720708e+105   6.18700571e+102]\n",
      "gradient: [  4.01950074e+110   9.99863431e+107]\n",
      "cost: 1.14893446339e+218\n",
      "loss: [ -1.50832139e+109  -1.51209217e+109  -1.51586295e+109  -1.51963373e+109\n",
      "  -1.52340451e+109]\n",
      "theta: [ -3.77078003e+106  -9.37993374e+103]\n",
      "gradient: [ -6.09384448e+111  -1.51586295e+109]\n",
      "cost: 2.64078937914e+220\n",
      "loss: [  2.28672081e+110   2.29243758e+110   2.29815435e+110   2.30387111e+110\n",
      "   2.30958788e+110]\n",
      "theta: [  5.71676648e+107   1.42206361e+105]\n",
      "gradient: [  9.23869480e+112   2.29815435e+110]\n",
      "cost: 6.06977052843e+222\n",
      "loss: [ -3.46682882e+111  -3.47549584e+111  -3.48416286e+111  -3.49282988e+111\n",
      "  -3.50149689e+111]\n",
      "theta: [ -8.66701816e+108  -2.15594798e+106]\n",
      "gradient: [ -1.40065080e+114  -3.48416286e+111]\n",
      "cost: 1.39511748112e+225\n",
      "loss: [  5.25595517e+112   5.26909498e+112   5.28223478e+112   5.29537459e+112\n",
      "   5.30851440e+112]\n",
      "theta: [  1.31398062e+110   3.26856806e+107]\n",
      "gradient: [  2.12348466e+115   5.28223478e+112]\n",
      "cost: 3.20663322772e+227\n",
      "loss: [ -7.96839596e+113  -7.98831682e+113  -8.00823769e+113  -8.02815855e+113\n",
      "  -8.04807942e+113]\n",
      "theta: [ -1.99208660e+111  -4.95537798e+108]\n",
      "gradient: [ -3.21935139e+116  -8.00823769e+113]\n",
      "cost: 7.37034464576e+229\n",
      "loss: [  1.20806461e+115   1.21108475e+115   1.21410489e+115   1.21712503e+115\n",
      "   1.22014518e+115]\n",
      "theta: [  3.02014273e+112   7.51269989e+109]\n",
      "gradient: [  4.88076207e+117   1.21410489e+115]\n",
      "cost: 1.69405031195e+232\n",
      "loss: [ -1.83151051e+116  -1.83608925e+116  -1.84066800e+116  -1.84524675e+116\n",
      "  -1.84982550e+116]\n",
      "theta: [ -4.57874779e+113  -1.13897789e+111]\n",
      "gradient: [ -7.39957694e+118  -1.84066800e+116]\n",
      "cost: 3.89372084664e+234\n",
      "loss: [  2.77669813e+117   2.78363984e+117   2.79058154e+117   2.79752324e+117\n",
      "   2.80446494e+117]\n",
      "theta: [  6.94170216e+114   1.72677021e+112]\n",
      "gradient: [  1.12182766e+120   2.79058154e+117]\n",
      "cost: 8.94959371905e+236\n",
      "loss: [ -4.20966874e+118  -4.22019285e+118  -4.23071695e+118  -4.24124106e+118\n",
      "  -4.25176516e+118]\n",
      "theta: [ -1.05241064e+116  -2.61790452e+113]\n",
      "gradient: [ -1.70076926e+121  -4.23071695e+118]\n",
      "cost: 2.05703569646e+239\n",
      "loss: [  6.38215248e+119   6.39810777e+119   6.41406305e+119   6.43001833e+119\n",
      "   6.44597361e+119]\n",
      "theta: [  1.59552820e+117   3.96892650e+114]\n",
      "gradient: [  2.57848526e+122   6.41406305e+119]\n",
      "cost: 4.72803122618e+241\n",
      "loss: [ -9.67578992e+120  -9.69997924e+120  -9.72416856e+120  -9.74835789e+120\n",
      "  -9.77254721e+120]\n",
      "theta: [ -2.41893244e+118  -6.01717040e+115]\n",
      "gradient: [ -3.90916414e+123  -9.72416856e+120]\n",
      "cost: 1.08672296325e+244\n",
      "loss: [  1.46691748e+122   1.47058475e+122   1.47425202e+122   1.47791929e+122\n",
      "   1.48158657e+122]\n",
      "theta: [  3.66727090e+119   9.12245152e+116]\n",
      "gradient: [  5.92656648e+124   1.47425202e+122]\n",
      "cost: 2.49779822162e+246\n",
      "loss: [ -2.22394959e+123  -2.22950943e+123  -2.23506927e+123  -2.24062910e+123\n",
      "  -2.24618894e+123]\n",
      "theta: [ -5.55983939e+120  -1.38302751e+118]\n",
      "gradient: [ -8.98508964e+125  -2.23506927e+123]\n",
      "cost: 5.74110989363e+248\n",
      "loss: [  3.37166325e+124   3.38009235e+124   3.38852146e+124   3.39695057e+124\n",
      "   3.40537967e+124]\n",
      "theta: [  8.42910570e+121   2.09676651e+119]\n",
      "gradient: [  1.36220249e+127   3.38852146e+124]\n",
      "cost: 1.31957587788e+251\n",
      "loss: [ -5.11167750e+125  -5.12445662e+125  -5.13723573e+125  -5.15001484e+125\n",
      "  -5.16279396e+125]\n",
      "theta: [ -1.27791143e+123  -3.17884481e+120]\n",
      "gradient: [ -2.06519432e+128  -5.13723573e+125]\n",
      "cost: 3.03300325156e+253\n",
      "loss: [  7.74966091e+126   7.76903494e+126   7.78840897e+126   7.80778300e+126\n",
      "   7.82715704e+126]\n",
      "theta: [  1.93740318e+124   4.81935125e+121]\n",
      "gradient: [  3.13097916e+129   7.78840897e+126]\n",
      "cost: 6.97126165924e+255\n",
      "loss: [ -1.17490284e+128  -1.17784008e+128  -1.18077732e+128  -1.18371456e+128\n",
      "  -1.18665180e+128]\n",
      "theta: [ -2.93723884e+125  -7.30647385e+122]\n",
      "gradient: [ -4.74678357e+130  -1.18077732e+128]\n",
      "cost: 1.6023223548e+258\n",
      "loss: [  1.78123495e+129   1.78568801e+129   1.79014107e+129   1.79459413e+129\n",
      "   1.79904719e+129]\n",
      "theta: [  4.45305968e+126   1.10771258e+124]\n",
      "gradient: [  7.19645616e+131   1.79014107e+129]\n",
      "cost: 3.68288705001e+260\n",
      "loss: [ -2.70047687e+130  -2.70722802e+130  -2.71397917e+130  -2.72073032e+130\n",
      "  -2.72748147e+130]\n",
      "theta: [ -6.75115019e+127  -1.67936981e+125]\n",
      "gradient: [ -1.09103313e+133  -2.71397917e+130]\n",
      "cost: 8.46499893262e+262\n",
      "loss: [  4.09411197e+131   4.10434719e+131   4.11458240e+131   4.12481762e+131\n",
      "   4.13505283e+131]\n",
      "theta: [  1.02352163e+129   2.54604219e+126]\n",
      "gradient: [  1.65408260e+134   4.11458240e+131]\n",
      "cost: 1.94565312366e+265\n",
      "loss: [ -6.20696033e+132  -6.22247764e+132  -6.23799494e+132  -6.25351225e+132\n",
      "  -6.26902955e+132]\n",
      "theta: [ -1.55173043e+130  -3.85997818e+127]\n",
      "gradient: [ -2.50770500e+135  -6.23799494e+132]\n",
      "cost: 4.47202191958e+267\n",
      "loss: [  9.41018635e+133   9.43371167e+133   9.45723699e+133   9.48076231e+133\n",
      "   9.50428763e+133]\n",
      "theta: [  2.35253196e+131   5.85199712e+128]\n",
      "gradient: [  3.80185632e+136   9.45723699e+133]\n",
      "cost: 1.0278800371e+270\n",
      "loss: [ -1.42665012e+135  -1.43021673e+135  -1.43378333e+135  -1.43734993e+135\n",
      "  -1.44091653e+135]\n",
      "theta: [ -3.56660313e+132  -8.87203728e+129]\n",
      "gradient: [ -5.76388031e+137  -1.43378333e+135]\n",
      "cost: 2.36254962445e+272\n",
      "loss: [  2.16290145e+136   2.16830867e+136   2.17371589e+136   2.17912311e+136\n",
      "   2.18453033e+136]\n",
      "theta: [  5.40722000e+133   1.34506296e+131]\n",
      "gradient: [  8.73844602e+138   2.17371589e+136]\n",
      "cost: 5.43024528791e+274\n",
      "loss: [ -3.27911000e+137  -3.28730773e+137  -3.29550545e+137  -3.30370317e+137\n",
      "  -3.31190090e+137]\n",
      "theta: [ -8.19772402e+134  -2.03920959e+132]\n",
      "gradient: [ -1.32480959e+140  -3.29550545e+137]\n",
      "cost: 1.24812463543e+277\n",
      "loss: [  4.97136030e+138   4.98378862e+138   4.99621695e+138   5.00864527e+138\n",
      "   5.02107359e+138]\n",
      "theta: [  1.24283235e+136   3.09158449e+133]\n",
      "gradient: [  2.00850407e+141   4.99621695e+138]\n",
      "cost: 2.86877483973e+279\n",
      "loss: [ -7.53693021e+139  -7.55577242e+139  -7.57461463e+139  -7.59345683e+139\n",
      "  -7.61229904e+139]\n",
      "theta: [ -1.88422083e+137  -4.68705850e+134]\n",
      "gradient: [ -3.04503276e+142  -7.57461463e+139]\n",
      "cost: 6.59378786982e+281\n",
      "loss: [  1.14265138e+141   1.14550799e+141   1.14836460e+141   1.15122121e+141\n",
      "   1.15407782e+141]\n",
      "theta: [  2.85661068e+138   7.10590878e+135]\n",
      "gradient: [  4.61648282e+143   1.14836460e+141]\n",
      "cost: 1.51556120299e+284\n",
      "loss: [ -1.73233947e+142  -1.73667030e+142  -1.74100112e+142  -1.74533194e+142\n",
      "  -1.74966276e+142]\n",
      "theta: [ -4.33082175e+139  -1.07730551e+137]\n",
      "gradient: [ -6.99891111e+144  -1.74100112e+142]\n",
      "cost: 3.48346929771e+286\n",
      "loss: [  2.62634791e+143   2.63291374e+143   2.63947957e+143   2.64604539e+143\n",
      "   2.65261122e+143]\n",
      "theta: [  6.56582894e+140   1.63327057e+138]\n",
      "gradient: [  1.06108392e+146   2.63947957e+143]\n",
      "cost: 8.00664356156e+288\n",
      "loss: [ -3.98172727e+144  -3.99168153e+144  -4.00163578e+144  -4.01159004e+144\n",
      "  -4.02154430e+144]\n",
      "theta: [ -9.95425628e+141  -2.47615251e+139]\n",
      "gradient: [ -1.60867749e+147  -4.00163578e+144]\n",
      "cost: 1.84030159715e+291\n",
      "loss: [  6.03657726e+145   6.05166861e+145   6.06675996e+145   6.08185131e+145\n",
      "   6.09694266e+145]\n",
      "theta: [  1.50913493e+143   3.75402053e+140]\n",
      "gradient: [  2.43886769e+148   6.06675996e+145]\n",
      "cost: 4.22987478139e+293\n",
      "loss: [ -9.15187369e+146  -9.17475323e+146  -9.19763278e+146  -9.22051232e+146\n",
      "  -9.24339186e+146]\n",
      "theta: [ -2.28795419e+144  -5.69135791e+141]\n",
      "gradient: [ -3.69749414e+149  -9.19763278e+146]\n",
      "cost: 9.72223286335e+295\n",
      "loss: [  1.38748811e+148   1.39095681e+148   1.39442551e+148   1.39789421e+148\n",
      "   1.40136291e+148]\n",
      "theta: [  3.46869872e+145   8.62849698e+142]\n",
      "gradient: [  5.60565993e+150   1.39442551e+148]\n",
      "cost: 2.23462435023e+298\n",
      "loss: [ -2.10352911e+149  -2.10878790e+149  -2.11404669e+149  -2.11930548e+149\n",
      "  -2.12456427e+149]\n",
      "theta: [ -5.25879006e+146  -1.30814054e+144]\n",
      "gradient: [ -8.49857285e+151  -2.11404669e+149]\n",
      "cost: 5.13621310744e+300\n",
      "loss: [  3.18909737e+150   3.19707007e+150   3.20504276e+150   3.21301545e+150\n",
      "   3.22098815e+150]\n",
      "theta: [  7.97269385e+147   1.98323263e+145]\n",
      "gradient: [  1.28844313e+153   3.20504276e+150]\n",
      "cost: 1.18054227246e+303\n",
      "loss: [ -4.83489485e+151  -4.84698201e+151  -4.85906918e+151  -4.87115634e+151\n",
      "  -4.88324350e+151]\n",
      "theta: [ -1.20871620e+149  -3.00671950e+146]\n",
      "gradient: [ -1.95336998e+154  -4.85906918e+151]\n",
      "cost: 2.71343892456e+305\n",
      "loss: [  7.33003904e+152   7.34836402e+152   7.36668900e+152   7.38501399e+152\n",
      "   7.40333897e+152]\n",
      "theta: [  1.83249836e+150   4.55839723e+147]\n",
      "gradient: [  2.96144563e+155   7.36668900e+152]\n",
      "cost: inf\n",
      "loss: [ -1.11128523e+154  -1.11406342e+154  -1.11684162e+154  -1.11961982e+154\n",
      "  -1.12239801e+154]\n",
      "theta: [ -2.77819579e+151  -6.91084928e+148]\n",
      "gradient: [ -4.48975888e+156  -1.11684162e+154]\n",
      "cost: inf\n",
      "loss: [  1.68478620e+155   1.68899813e+155   1.69321007e+155   1.69742201e+155\n",
      "   1.70163395e+155]\n",
      "theta: [  4.21193930e+152   1.04773313e+150]\n",
      "gradient: [  6.80678874e+157   1.69321007e+155]\n",
      "cost: inf\n",
      "loss: [ -2.55425381e+156  -2.56063940e+156  -2.56702500e+156  -2.57341059e+156\n",
      "  -2.57979619e+156]\n",
      "theta: [ -6.38559481e+153  -1.58843676e+151]\n",
      "gradient: [ -1.03195682e+159  -2.56702500e+156]\n",
      "cost: inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mimi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:10: RuntimeWarning: overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss: [  3.87242757e+157   3.88210858e+157   3.89178959e+157   3.90147060e+157\n",
      "   3.91115160e+157]\n",
      "theta: [  9.68100872e+154   2.40818132e+152]\n",
      "gradient: [  1.56451878e+160   3.89178959e+157]\n",
      "cost: inf\n",
      "loss: [ -5.87087127e+158  -5.88554835e+158  -5.90022544e+158  -5.91490253e+158\n",
      "  -5.92957961e+158]\n",
      "theta: [ -1.46770869e+156  -3.65097145e+153]\n",
      "gradient: [ -2.37191998e+161  -5.90022544e+158]\n",
      "cost: inf\n",
      "loss: [  8.90065180e+159   8.92290329e+159   8.94515478e+159   8.96740627e+159\n",
      "   8.98965776e+159]\n",
      "theta: [  2.22514911e+157   5.53512829e+154]\n",
      "gradient: [  3.59599672e+162   8.94515478e+159]\n",
      "cost: inf\n",
      "loss: [ -1.34940112e+161  -1.35277460e+161  -1.35614808e+161  -1.35952156e+161\n",
      "  -1.36289504e+161]\n",
      "theta: [ -3.37348181e+158  -8.39164195e+155]\n",
      "gradient: [ -5.45178275e+163  -1.35614808e+161]\n",
      "cost: inf\n",
      "loss: [  2.04578655e+162   2.05090099e+162   2.05601542e+162   2.06112986e+162\n",
      "   2.06624429e+162]\n",
      "theta: [  5.11443457e+159   1.27223166e+157]\n",
      "gradient: [  8.26528428e+164   2.05601542e+162]\n",
      "cost: inf\n",
      "loss: [ -3.10155562e+163  -3.10930946e+163  -3.11706330e+163  -3.12481714e+163\n",
      "  -3.13257098e+163]\n",
      "theta: [ -7.75384082e+160  -1.92879225e+158]\n",
      "gradient: [ -1.25307495e+166  -3.11706330e+163]\n",
      "cost: inf\n",
      "loss: [  4.70217542e+164   4.71393079e+164   4.72568615e+164   4.73744152e+164\n",
      "   4.74919689e+164]\n",
      "theta: [  1.17553655e+162   2.92418407e+159]\n",
      "gradient: [  1.89974934e+167   4.72568615e+164]\n",
      "cost: inf\n",
      "loss: [ -7.12882709e+165  -7.14664905e+165  -7.16447101e+165  -7.18229296e+165\n",
      "  -7.20011492e+165]\n",
      "theta: [ -1.78219569e+163  -4.43326775e+160]\n",
      "gradient: [ -2.88015299e+168  -7.16447101e+165]\n",
      "cost: inf\n",
      "loss: [  1.08078009e+167   1.08348202e+167   1.08618396e+167   1.08888589e+167\n",
      "   1.09158782e+167]\n",
      "theta: [  2.70193342e+164   6.72114423e+161]\n",
      "gradient: [  4.36651354e+169   1.08618396e+167]\n",
      "cost: inf\n",
      "loss: [ -1.63853827e+168  -1.64263459e+168  -1.64673091e+168  -1.65082723e+168\n",
      "  -1.65492355e+168]\n",
      "theta: [ -4.09632020e+165  -1.01897251e+163]\n",
      "gradient: [ -6.61994018e+170  -1.64673091e+168]\n",
      "cost: inf\n",
      "loss: [  2.48413871e+169   2.49034902e+169   2.49655933e+169   2.50276964e+169\n",
      "   2.50897995e+169]\n",
      "theta: [  6.21030816e+166   1.54483366e+164]\n",
      "gradient: [  1.00362927e+172   2.49655933e+169]\n",
      "cost: inf\n",
      "loss: [ -3.76612818e+170  -3.77554344e+170  -3.78495870e+170  -3.79437397e+170\n",
      "  -3.80378923e+170]\n",
      "theta: [ -9.41526190e+167  -2.34207596e+165]\n",
      "gradient: [ -1.52157223e+173  -3.78495870e+170]\n",
      "cost: inf\n",
      "loss: [  5.70971395e+171   5.72398815e+171   5.73826234e+171   5.75253654e+171\n",
      "   5.76681073e+171]\n",
      "theta: [  1.42741961e+169   3.55075111e+166]\n",
      "gradient: [  2.30681001e+174   5.73826234e+171]\n",
      "cost: inf\n",
      "loss: [ -8.65632603e+172  -8.67796671e+172  -8.69960739e+172  -8.72124807e+172\n",
      "  -8.74288875e+172]\n",
      "theta: [ -2.16406805e+170  -5.38318723e+167]\n",
      "gradient: [ -3.49728545e+175  -8.69960739e+172]\n",
      "cost: inf\n",
      "loss: [  1.31235962e+174   1.31564050e+174   1.31892138e+174   1.32220226e+174\n",
      "   1.32548313e+174]\n",
      "theta: [  3.28087865e+171   8.16128866e+168]\n",
      "gradient: [  5.30212955e+176   1.31892138e+174]\n",
      "cost: inf\n",
      "loss: [ -1.98962905e+175  -1.99460309e+175  -1.99957713e+175  -2.00455117e+175\n",
      "  -2.00952522e+175]\n",
      "theta: [ -4.97404169e+172  -1.23730849e+170]\n",
      "gradient: [ -8.03839955e+177  -1.99957713e+175]\n",
      "cost: inf\n",
      "loss: [  3.01641691e+176   3.02395791e+176   3.03149890e+176   3.03903990e+176\n",
      "   3.04658089e+176]\n",
      "theta: [  7.54099538e+173   1.87584628e+171]\n",
      "gradient: [  1.21867764e+179   3.03149890e+176]\n",
      "cost: inf\n",
      "loss: [ -4.57309919e+177  -4.58453186e+177  -4.59596454e+177  -4.60739722e+177\n",
      "  -4.61882989e+177]\n",
      "theta: [ -1.14326769e+175  -2.84391427e+172]\n",
      "gradient: [ -1.84760061e+180  -4.59596454e+177]\n",
      "cost: inf\n",
      "loss: [  6.93313848e+178   6.95047122e+178   6.96780396e+178   6.98513670e+178\n",
      "   7.00246944e+178]\n",
      "theta: [  1.73327384e+176   4.31157311e+173]\n",
      "gradient: [  2.80109186e+181   6.96780396e+178]\n",
      "cost: inf\n",
      "loss: [ -1.05111233e+180  -1.05374009e+180  -1.05636785e+180  -1.05899562e+180\n",
      "  -1.06162338e+180]\n",
      "theta: [ -2.62776447e+177  -6.53664665e+174]\n",
      "gradient: [ -4.24665133e+182  -1.05636785e+180]\n",
      "cost: inf\n",
      "loss: [  1.59355986e+181   1.59754374e+181   1.60152761e+181   1.60551149e+181\n",
      "   1.60949536e+181]\n",
      "theta: [  3.98387488e+178   9.91001388e+175]\n",
      "gradient: [  6.43822068e+183   1.60152761e+181]\n",
      "cost: inf\n",
      "loss: [ -2.41594830e+182  -2.42198814e+182  -2.42802797e+182  -2.43406780e+182\n",
      "  -2.44010764e+182]\n",
      "theta: [ -6.03983320e+179  -1.50242747e+177]\n",
      "gradient: [ -9.76079323e+184  -2.42802797e+182]\n",
      "cost: inf\n",
      "loss: [  3.66274674e+183   3.67190355e+183   3.68106036e+183   3.69021717e+183\n",
      "   3.69937398e+183]\n",
      "theta: [  9.15680991e+180   2.27778522e+178]\n",
      "gradient: [  1.47980458e+186   3.68106036e+183]\n",
      "cost: inf\n",
      "loss: [ -5.55298045e+184  -5.56686282e+184  -5.58074518e+184  -5.59462755e+184\n",
      "  -5.60850991e+184]\n",
      "theta: [ -1.38823648e+182  -3.45328184e+179]\n",
      "gradient: [ -2.24348733e+187  -5.58074518e+184]\n",
      "cost: inf\n",
      "loss: [  8.41870708e+185   8.43975371e+185   8.46080035e+185   8.48184699e+185\n",
      "   8.50289362e+185]\n",
      "theta: [  2.10466368e+183   5.23541700e+180]\n",
      "gradient: [  3.40128383e+188   8.46080035e+185]\n",
      "cost: inf\n",
      "loss: [ -1.27633492e+187  -1.27952574e+187  -1.28271656e+187  -1.28590738e+187\n",
      "  -1.28909819e+187]\n",
      "theta: [ -3.19081747e+184  -7.93725865e+181]\n",
      "gradient: [ -5.15658438e+189  -1.28271656e+187]\n",
      "cost: inf\n",
      "loss: [  1.93501309e+188   1.93985059e+188   1.94468809e+188   1.94952560e+188\n",
      "   1.95436310e+188]\n",
      "theta: [  4.83750263e+185   1.20334397e+183]\n",
      "gradient: [  7.81774288e+190   1.94468809e+188]\n",
      "cost: inf\n",
      "loss: [ -2.93361529e+189  -2.94094928e+189  -2.94828328e+189  -2.95561727e+189\n",
      "  -2.96295126e+189]\n",
      "theta: [ -7.33399262e+186  -1.82435370e+184]\n",
      "gradient: [ -1.18522455e+192  -2.94828328e+189]\n",
      "cost: inf\n",
      "loss: [  4.44756613e+190   4.45868498e+190   4.46980383e+190   4.48092267e+190\n",
      "   4.49204152e+190]\n",
      "theta: [  1.11188462e+188   2.76584791e+185]\n",
      "gradient: [  1.79688338e+193   4.46980383e+190]\n",
      "cost: inf\n",
      "loss: [ -6.74282159e+191  -6.75967854e+191  -6.77653549e+191  -6.79339244e+191\n",
      "  -6.81024939e+191]\n",
      "theta: [ -1.68569491e+189  -4.19321904e+186]\n",
      "gradient: [ -2.72420098e+194  -6.77653549e+191]\n",
      "cost: inf\n",
      "loss: [  1.02225895e+193   1.02481458e+193   1.02737022e+193   1.02992585e+193\n",
      "   1.03248148e+193]\n",
      "theta: [  2.55563149e+190   6.35721358e+187]\n",
      "gradient: [  4.13007938e+195   1.02737022e+193]\n",
      "cost: inf\n",
      "loss: [ -1.54981613e+194  -1.55369065e+194  -1.55756516e+194  -1.56143968e+194\n",
      "  -1.56531419e+194]\n",
      "theta: [ -3.87451623e+191  -9.63798080e+188]\n",
      "gradient: [ -6.26148944e+196  -1.55756516e+194]\n",
      "cost: inf\n",
      "loss: [  2.34962974e+195   2.35550378e+195   2.36137782e+195   2.36725185e+195\n",
      "   2.37312589e+195]\n",
      "theta: [  5.87403782e+192   1.46118535e+190]\n",
      "gradient: [  9.49285630e+197   2.36137782e+195]\n",
      "cost: inf\n",
      "loss: [ -3.56220316e+196  -3.57110861e+196  -3.58001406e+196  -3.58891952e+196\n",
      "  -3.59782497e+196]\n",
      "theta: [ -8.90545252e+193  -2.21525928e+191]\n",
      "gradient: [ -1.43918346e+199  -3.58001406e+196]\n",
      "cost: inf\n",
      "loss: [  5.40054934e+197   5.41405063e+197   5.42755192e+197   5.44105321e+197\n",
      "   5.45455450e+197]\n",
      "theta: [  1.35012894e+195   3.35848814e+192]\n",
      "gradient: [  2.18190288e+200   5.42755192e+197]\n",
      "cost: inf\n",
      "loss: [ -8.18761084e+198  -8.20807974e+198  -8.22854864e+198  -8.24901754e+198\n",
      "  -8.26948644e+198]\n",
      "theta: [ -2.04688998e+196  -5.09170311e+193]\n",
      "gradient: [ -3.30791749e+201  -8.22854864e+198]\n",
      "cost: inf\n",
      "loss: [  1.24129912e+200   1.24440235e+200   1.24750557e+200   1.25060880e+200\n",
      "   1.25371203e+200]\n",
      "theta: [  3.10322849e+197   7.71937833e+194]\n",
      "gradient: [  5.01503447e+202   1.24750557e+200]\n",
      "cost: inf\n",
      "loss: [ -1.88189635e+201  -1.88660106e+201  -1.89130578e+201  -1.89601049e+201\n",
      "  -1.90071520e+201]\n",
      "theta: [ -4.70471162e+198  -1.17031179e+196]\n",
      "gradient: [ -7.60314331e+203  -1.89130578e+201]\n",
      "cost: inf\n",
      "loss: [  2.85308660e+202   2.86021927e+202   2.86735195e+202   2.87448462e+202\n",
      "   2.88161729e+202]\n",
      "theta: [  7.13267215e+199   1.77427460e+197]\n",
      "gradient: [  1.15268975e+205   2.86735195e+202]\n",
      "cost: inf\n",
      "loss: [ -4.32547900e+203  -4.33629263e+203  -4.34710627e+203  -4.35791990e+203\n",
      "  -4.36873353e+203]\n",
      "theta: [ -1.08136303e+201  -2.68992449e+198]\n",
      "gradient: [ -1.74755835e+206  -4.34710627e+203]\n",
      "cost: inf\n",
      "loss: [  6.55772895e+204   6.57412317e+204   6.59051740e+204   6.60691162e+204\n",
      "   6.62330584e+204]\n",
      "theta: [  1.63942204e+202   4.07811382e+199]\n",
      "gradient: [  2.64942078e+207   6.59051740e+204]\n",
      "cost: inf\n",
      "loss: [ -9.94197613e+205  -9.96683092e+205  -9.99168571e+205  -1.00165405e+206\n",
      "  -1.00413953e+206]\n",
      "theta: [ -2.48547858e+203  -6.18270601e+200]\n",
      "gradient: [ -4.01670736e+208  -9.99168571e+205]\n",
      "cost: inf\n",
      "loss: [  1.50727318e+207   1.51104134e+207   1.51480949e+207   1.51857765e+207\n",
      "   1.52234581e+207]\n",
      "theta: [  3.76815951e+204   9.37341510e+201]\n",
      "gradient: [  6.08960953e+209   1.51480949e+207]\n",
      "cost: inf\n",
      "loss: [ -2.28513164e+208  -2.29084444e+208  -2.29655723e+208  -2.30227002e+208\n",
      "  -2.30798282e+208]\n",
      "theta: [ -5.71279358e+205  -1.42107534e+203]\n",
      "gradient: [ -9.23227432e+210  -2.29655723e+208]\n",
      "cost: inf\n",
      "loss: [  3.46441953e+209   3.47308053e+209   3.48174152e+209   3.49040252e+209\n",
      "   3.49906351e+209]\n",
      "theta: [  8.66099496e+206   2.15444970e+204]\n",
      "gradient: [  1.39967741e+212   3.48174152e+209]\n",
      "cost: inf\n",
      "loss: [ -5.25230252e+210  -5.26543319e+210  -5.27856387e+210  -5.29169454e+210\n",
      "  -5.30482522e+210]\n",
      "theta: [ -1.31306746e+208  -3.26629655e+205]\n",
      "gradient: [ -2.12200894e+213  -5.27856387e+210]\n",
      "cost: inf\n",
      "loss: [  7.96285828e+211   7.98276530e+211   8.00267232e+211   8.02257934e+211\n",
      "   8.04248636e+211]\n",
      "theta: [  1.99070219e+209   4.95193421e+206]\n",
      "gradient: [  3.21711409e+214   8.00267232e+211]\n",
      "cost: inf\n",
      "loss: [ -1.20722505e+213  -1.21024310e+213  -1.21326114e+213  -1.21627919e+213\n",
      "  -1.21929723e+213]\n",
      "theta: [ -3.01804387e+210  -7.50747890e+207]\n",
      "gradient: [ -4.87737015e+215  -1.21326114e+213]\n",
      "cost: inf\n",
      "loss: [  1.83023769e+214   1.83481325e+214   1.83938882e+214   1.84396439e+214\n",
      "   1.84853995e+214]\n",
      "theta: [  4.57556577e+211   1.13818635e+209]\n",
      "gradient: [  7.39443457e+216   1.83938882e+214]\n",
      "cost: inf\n",
      "loss: [ -2.77476845e+215  -2.78170533e+215  -2.78864221e+215  -2.79557909e+215\n",
      "  -2.80251596e+215]\n",
      "theta: [ -6.93687799e+212  -1.72557018e+210]\n",
      "gradient: [ -1.12104804e+218  -2.78864221e+215]\n",
      "cost: inf\n",
      "loss: [  4.20674321e+216   4.21726000e+216   4.22777679e+216   4.23829358e+216\n",
      "   4.24881038e+216]\n",
      "theta: [  1.05167926e+214   2.61608519e+211]\n",
      "gradient: [  1.69958730e+219   4.22777679e+216]\n",
      "cost: inf\n",
      "loss: [ -6.37771717e+217  -6.39366137e+217  -6.40960556e+217  -6.42554975e+217\n",
      "  -6.44149395e+217]\n",
      "theta: [ -1.59441938e+215  -3.96616827e+212]\n",
      "gradient: [ -2.57669332e+220  -6.40960556e+217]\n",
      "cost: inf\n",
      "loss: [  9.66906567e+218   9.69323819e+218   9.71741070e+218   9.74158321e+218\n",
      "   9.76575573e+218]\n",
      "theta: [  2.41725139e+216   6.01298873e+213]\n",
      "gradient: [  3.90644745e+221   9.71741070e+218]\n",
      "cost: inf\n",
      "loss: [ -1.46589804e+220  -1.46956276e+220  -1.47322748e+220  -1.47689221e+220\n",
      "  -1.48055693e+220]\n",
      "theta: [ -3.66472231e+217  -9.11611183e+214]\n",
      "gradient: [ -5.92244778e+222  -1.47322748e+220]\n",
      "cost: inf\n",
      "loss: [  2.22240404e+221   2.22796002e+221   2.23351599e+221   2.23907197e+221\n",
      "   2.24462794e+221]\n",
      "theta: [  5.55597555e+218   1.38206637e+216]\n",
      "gradient: [  8.97884541e+223   2.23351599e+221]\n",
      "cost: inf\n",
      "loss: [ -3.36932009e+222  -3.37774334e+222  -3.38616659e+222  -3.39458984e+222\n",
      "  -3.40301308e+222]\n",
      "theta: [ -8.42324785e+219  -2.09530935e+217]\n",
      "gradient: [ -1.36125582e+225  -3.38616659e+222]\n",
      "cost: inf\n",
      "loss: [  5.10812511e+223   5.12089535e+223   5.13366558e+223   5.14643581e+223\n",
      "   5.15920605e+223]\n",
      "theta: [  1.27702334e+221   3.17663565e+218]\n",
      "gradient: [  2.06375910e+226   5.13366558e+223]\n",
      "cost: inf\n",
      "loss: [ -7.74427524e+224  -7.76363581e+224  -7.78299638e+224  -7.80235694e+224\n",
      "  -7.82171751e+224]\n",
      "theta: [ -1.93605677e+222  -4.81600201e+219]\n",
      "gradient: [ -3.12880326e+227  -7.78299638e+224]\n",
      "cost: inf\n",
      "loss: [  1.17408634e+226   1.17702153e+226   1.17995673e+226   1.18289193e+226\n",
      "   1.18582713e+226]\n",
      "theta: [  2.93519759e+223   7.30139617e+220]\n",
      "gradient: [  4.74348476e+228   1.17995673e+226]\n",
      "cost: inf\n",
      "loss: [ -1.77999707e+227  -1.78444704e+227  -1.78889700e+227  -1.79334697e+227\n",
      "  -1.79779693e+227]\n",
      "theta: [ -4.44996501e+224  -1.10694277e+222]\n",
      "gradient: [ -7.19145495e+229  -1.78889700e+227]\n",
      "cost: inf\n",
      "loss: [  2.69860016e+228   2.70534662e+228   2.71209308e+228   2.71883954e+228\n",
      "   2.72558599e+228]\n",
      "theta: [  6.74645845e+225   1.67820272e+223]\n",
      "gradient: [  1.09027491e+231   2.71209308e+228]\n",
      "cost: inf\n",
      "loss: [ -4.09126674e+229  -4.10149485e+229  -4.11172295e+229  -4.12195105e+229\n",
      "  -4.13217916e+229]\n",
      "theta: [ -1.02281033e+227  -2.54427280e+224]\n",
      "gradient: [ -1.65293308e+232  -4.11172295e+229]\n",
      "cost: inf\n",
      "loss: [  6.20264677e+230   6.21815329e+230   6.23365981e+230   6.24916633e+230\n",
      "   6.26467285e+230]\n",
      "theta: [  1.55065205e+228   3.85729567e+225]\n",
      "gradient: [  2.50596226e+233   6.23365981e+230]\n",
      "cost: inf\n",
      "loss: [ -9.40364669e+231  -9.42715566e+231  -9.45066463e+231  -9.47417360e+231\n",
      "  -9.49768257e+231]\n",
      "theta: [ -2.35089705e+229  -5.84793025e+226]\n",
      "gradient: [ -3.79921420e+234  -9.45066463e+231]\n",
      "cost: inf\n",
      "loss: [  1.42565866e+233   1.42922279e+233   1.43278691e+233   1.43635104e+233\n",
      "   1.43991516e+233]\n",
      "theta: [  3.56412450e+230   8.86587161e+227]\n",
      "gradient: [  5.75987467e+235   1.43278691e+233]\n",
      "cost: inf\n",
      "loss: [ -2.16139833e+234  -2.16680179e+234  -2.17220525e+234  -2.17760872e+234\n",
      "  -2.18301218e+234]\n",
      "theta: [ -5.40346222e+231  -1.34412820e+229]\n",
      "gradient: [ -8.73237319e+236  -2.17220525e+234]\n",
      "cost: inf\n",
      "loss: [  3.27683117e+235   3.28502319e+235   3.29321522e+235   3.30140725e+235\n",
      "   3.30959927e+235]\n",
      "theta: [  8.19202697e+232   2.03779244e+230]\n",
      "gradient: [  1.32388890e+238   3.29321522e+235]\n",
      "cost: inf\n",
      "loss: [ -4.96790543e+236  -4.98032511e+236  -4.99274480e+236  -5.00516449e+236\n",
      "  -5.01758417e+236]\n",
      "theta: [ -1.24196863e+234  -3.08943598e+231]\n",
      "gradient: [ -2.00710825e+239  -4.99274480e+236]\n",
      "cost: inf\n",
      "loss: [  7.53169238e+237   7.55052149e+237   7.56935061e+237   7.58817972e+237\n",
      "   7.60700883e+237]\n",
      "theta: [  1.88291139e+235   4.68380120e+232]\n",
      "gradient: [  3.04291660e+240   7.56935061e+237]\n",
      "cost: inf\n",
      "loss: [ -1.14185729e+239  -1.14471191e+239  -1.14756654e+239  -1.15042116e+239\n",
      "  -1.15327579e+239]\n",
      "theta: [ -2.85462546e+236  -7.10097049e+233]\n",
      "gradient: [ -4.61327457e+241  -1.14756654e+239]\n",
      "cost: inf\n",
      "loss: [  1.73113558e+240   1.73546339e+240   1.73979120e+240   1.74411901e+240\n",
      "   1.74844682e+240]\n",
      "theta: [  4.32781203e+237   1.07655683e+235]\n",
      "gradient: [  6.99404718e+242   1.73979120e+240]\n",
      "cost: inf\n",
      "loss: [ -2.62452271e+241  -2.63108398e+241  -2.63764524e+241  -2.64420651e+241\n",
      "  -2.65076778e+241]\n",
      "theta: [ -6.56126598e+238  -1.63213552e+236]\n",
      "gradient: [ -1.06034651e+244  -2.63764524e+241]\n",
      "cost: inf\n",
      "loss: [  3.97896015e+242   3.98890749e+242   3.99885483e+242   4.00880216e+242\n",
      "   4.01874950e+242]\n",
      "theta: [  9.94733851e+239   2.47443169e+237]\n",
      "gradient: [  1.60755953e+245   3.99885483e+242]\n",
      "cost: inf\n",
      "loss: [ -6.03238211e+243  -6.04746297e+243  -6.06254383e+243  -6.07762470e+243\n",
      "  -6.09270556e+243]\n",
      "theta: [ -1.50808615e+241  -3.75141166e+238]\n",
      "gradient: [ -2.43717278e+246  -6.06254383e+243]\n",
      "cost: inf\n",
      "loss: [  9.14551355e+244   9.16837719e+244   9.19124083e+244   9.21410447e+244\n",
      "   9.23696811e+244]\n",
      "theta: [  2.28636417e+242   5.68740267e+239]\n",
      "gradient: [  3.69492454e+247   9.19124083e+244]\n",
      "cost: inf\n",
      "loss: [ -1.38652387e+246  -1.38999016e+246  -1.39345645e+246  -1.39692274e+246\n",
      "  -1.40038902e+246]\n",
      "theta: [ -3.46628812e+243  -8.62250056e+240]\n",
      "gradient: [ -5.60176425e+248  -1.39345645e+246]\n",
      "cost: inf\n",
      "loss: [  2.10206725e+247   2.10732238e+247   2.11257752e+247   2.11783265e+247\n",
      "   2.12308779e+247]\n",
      "theta: [  5.25513544e+244   1.30723144e+242]\n",
      "gradient: [  8.49266672e+249   2.11257752e+247]\n",
      "cost: inf\n",
      "loss: [ -3.18688109e+248  -3.19484824e+248  -3.20281540e+248  -3.21078255e+248\n",
      "  -3.21874970e+248]\n",
      "theta: [ -7.96715318e+245  -1.98185437e+243]\n",
      "gradient: [ -1.28754772e+251  -3.20281540e+248]\n",
      "cost: inf\n",
      "loss: [  4.83153481e+249   4.84361358e+249   4.85569234e+249   4.86777110e+249\n",
      "   4.87984986e+249]\n",
      "theta: [  1.20787619e+247   3.00462996e+244]\n",
      "gradient: [  1.95201248e+252   4.85569234e+249]\n",
      "cost: inf\n",
      "loss: [ -7.32494499e+250  -7.34325723e+250  -7.36156948e+250  -7.37988173e+250\n",
      "  -7.39819398e+250]\n",
      "theta: [ -1.83122486e+248  -4.55522934e+245]\n",
      "gradient: [ -2.95938756e+253  -7.36156948e+250]\n",
      "cost: inf\n",
      "loss: [  1.11051293e+252   1.11328920e+252   1.11606546e+252   1.11884173e+252\n",
      "   1.12161799e+252]\n",
      "theta: [  2.77626507e+249   6.90604655e+246]\n",
      "gradient: [  4.48663869e+254   1.11606546e+252]\n",
      "cost: inf\n",
      "loss: [ -1.68361534e+253  -1.68782436e+253  -1.69203337e+253  -1.69624238e+253\n",
      "  -1.70045139e+253]\n",
      "theta: [ -4.20901219e+250  -1.04700500e+248]\n",
      "gradient: [ -6.80205832e+255  -1.69203337e+253]\n",
      "cost: inf\n",
      "loss: [  2.55247871e+254   2.55885987e+254   2.56524103e+254   2.57162219e+254\n",
      "   2.57800334e+254]\n",
      "theta: [  6.38115710e+251   1.58733287e+249]\n",
      "gradient: [  1.03123966e+257   2.56524103e+254]\n",
      "cost: inf\n",
      "loss: [ -3.86973640e+255  -3.87941069e+255  -3.88908497e+255  -3.89875925e+255\n",
      "  -3.90843353e+255]\n",
      "theta: [ -9.67428085e+252  -2.40650774e+250]\n",
      "gradient: [ -1.56343151e+258  -3.88908497e+255]\n",
      "cost: inf\n",
      "loss: [  5.86679127e+256   5.88145816e+256   5.89612504e+256   5.91079193e+256\n",
      "   5.92545882e+256]\n",
      "theta: [  1.46668870e+254   3.64843419e+251]\n",
      "gradient: [  2.37027160e+259   5.89612504e+256]\n",
      "cost: inf\n",
      "loss: [ -8.89446624e+257  -8.91670227e+257  -8.93893830e+257  -8.96117432e+257\n",
      "  -8.98341035e+257]\n",
      "theta: [ -2.22360273e+255  -5.53128163e+252]\n",
      "gradient: [ -3.59349767e+260  -8.93893830e+257]\n",
      "cost: inf\n",
      "loss: [  1.34846334e+259   1.35183448e+259   1.35520562e+259   1.35857676e+259\n",
      "   1.36194789e+259]\n",
      "theta: [  3.37113739e+256   8.38581013e+253]\n",
      "gradient: [  5.44799401e+261   1.35520562e+259]\n",
      "cost: inf\n",
      "loss: [ -2.04436482e+260  -2.04947570e+260  -2.05458658e+260  -2.05969746e+260\n",
      "  -2.06480834e+260]\n",
      "theta: [ -5.11088027e+257  -1.27134752e+255]\n",
      "gradient: [ -8.25954027e+262  -2.05458658e+260]\n",
      "cost: inf\n",
      "loss: [  3.09940017e+261   3.10714863e+261   3.11489708e+261   3.12264553e+261\n",
      "   3.13039398e+261]\n",
      "theta: [  7.74845225e+258   1.92745183e+256]\n",
      "gradient: [  1.25220412e+264   3.11489708e+261]\n",
      "cost: inf\n",
      "loss: [ -4.69890762e+262  -4.71065482e+262  -4.72240201e+262  -4.73414921e+262\n",
      "  -4.74589640e+262]\n",
      "theta: [ -1.17471960e+260  -2.92215190e+257]\n",
      "gradient: [ -1.89842910e+265  -4.72240201e+262]\n",
      "cost: inf\n",
      "loss: [  7.12387288e+263   7.14168245e+263   7.15949202e+263   7.17730159e+263\n",
      "   7.19511116e+263]\n",
      "theta: [  1.78095714e+261   4.43018682e+258]\n",
      "gradient: [  2.87815141e+266   7.15949202e+263]\n",
      "cost: inf\n",
      "loss: [ -1.08002900e+265  -1.08272905e+265  -1.08542911e+265  -1.08812916e+265\n",
      "  -1.09082922e+265]\n",
      "theta: [ -2.70005570e+262  -6.71647334e+259]\n",
      "gradient: [ -4.36347901e+267  -1.08542911e+265]\n",
      "cost: inf\n",
      "loss: [  1.63739956e+266   1.64149303e+266   1.64558651e+266   1.64967998e+266\n",
      "   1.65377345e+266]\n",
      "theta: [  4.09347344e+263   1.01826437e+261]\n",
      "gradient: [  6.61533962e+268   1.64558651e+266]\n",
      "cost: inf\n",
      "loss: [ -2.48241235e+267  -2.48861834e+267  -2.49482433e+267  -2.50103033e+267\n",
      "  -2.50723632e+267]\n",
      "theta: [ -6.20599228e+264  -1.54376007e+262]\n",
      "gradient: [ -1.00293179e+270  -2.49482433e+267]\n",
      "cost: inf\n",
      "loss: [  3.76351089e+268   3.77291961e+268   3.78232833e+268   3.79173704e+268\n",
      "   3.80114576e+268]\n",
      "theta: [  9.40871871e+265   2.34044833e+263]\n",
      "gradient: [  1.52051480e+271   3.78232833e+268]\n",
      "cost: inf\n",
      "loss: [ -5.70574595e+269  -5.72001023e+269  -5.73427450e+269  -5.74853878e+269\n",
      "  -5.76280306e+269]\n",
      "theta: [ -1.42642762e+267  -3.54828349e+264]\n",
      "gradient: [ -2.30520688e+272  -5.73427450e+269]\n",
      "cost: inf\n",
      "loss: [  8.65031027e+270   8.67193591e+270   8.69356155e+270   8.71518719e+270\n",
      "   8.73681283e+270]\n",
      "theta: [  2.16256412e+268   5.37944616e+265]\n",
      "gradient: [  3.49485499e+273   8.69356155e+270]\n",
      "cost: inf\n",
      "loss: [ -1.31144759e+272  -1.31472619e+272  -1.31800479e+272  -1.32128338e+272\n",
      "  -1.32456198e+272]\n",
      "theta: [ -3.27859858e+269  -8.15561693e+266]\n",
      "gradient: [ -5.29844481e+274  -1.31800479e+272]\n",
      "cost: inf\n",
      "loss: [  1.98824634e+273   1.99321693e+273   1.99818751e+273   2.00315810e+273\n",
      "   2.00812868e+273]\n",
      "theta: [  4.97058495e+270   1.23644862e+268]\n",
      "gradient: [  8.03281322e+275   1.99818751e+273]\n",
      "cost: inf\n",
      "loss: [ -3.01432064e+274  -3.02185639e+274  -3.02939215e+274  -3.03692790e+274\n",
      "  -3.04446365e+274]\n",
      "theta: [ -7.53575473e+271  -1.87454265e+269]\n",
      "gradient: [ -1.21783071e+277  -3.02939215e+274]\n",
      "cost: inf\n",
      "loss: [  4.56992109e+275   4.58134582e+275   4.59277055e+275   4.60419528e+275\n",
      "   4.61562001e+275]\n",
      "theta: [  1.14247317e+273   2.84193788e+270]\n",
      "gradient: [  1.84631661e+278   4.59277055e+275]\n",
      "cost: inf\n",
      "loss: [ -6.92832026e+276  -6.94564095e+276  -6.96296165e+276  -6.98028234e+276\n",
      "  -6.99760303e+276]\n",
      "theta: [ -1.73206929e+274  -4.30857676e+271]\n",
      "gradient: [ -2.79914522e+279  -6.96296165e+276]\n",
      "cost: inf\n",
      "loss: [  1.05038185e+278   1.05300779e+278   1.05563373e+278   1.05825966e+278\n",
      "   1.06088560e+278]\n",
      "theta: [  2.62593829e+275   6.53210397e+272]\n",
      "gradient: [  4.24370010e+280   1.05563373e+278]\n",
      "cost: inf\n",
      "loss: [ -1.59245241e+279  -1.59643352e+279  -1.60041462e+279  -1.60439573e+279\n",
      "  -1.60837684e+279]\n",
      "theta: [ -3.98110627e+276  -9.90312687e+273]\n",
      "gradient: [ -6.43374641e+281  -1.60041462e+279]\n",
      "cost: inf\n",
      "loss: [  2.41426933e+280   2.42030496e+280   2.42634060e+280   2.43237623e+280\n",
      "   2.43841187e+280]\n",
      "theta: [  6.03563578e+277   1.50138335e+275]\n",
      "gradient: [  9.75400992e+282   2.42634060e+280]\n",
      "cost: inf\n",
      "loss: [ -3.66020130e+281  -3.66935174e+281  -3.67850219e+281  -3.68765264e+281\n",
      "  -3.69680308e+281]\n",
      "theta: [ -9.15044634e+278  -2.27620226e+276]\n",
      "gradient: [ -1.47877618e+284  -3.67850219e+281]\n",
      "cost: inf\n",
      "loss: [  5.54912138e+282   5.56299410e+282   5.57686681e+282   5.59073953e+282\n",
      "   5.60461225e+282]\n",
      "theta: [  1.38727172e+280   3.45088196e+277]\n",
      "gradient: [  2.24192820e+285   5.57686681e+282]\n",
      "cost: inf\n",
      "loss: [ -8.41285645e+283  -8.43388846e+283  -8.45492047e+283  -8.47595248e+283\n",
      "  -8.49698449e+283]\n",
      "theta: [ -2.10320103e+281  -5.23177862e+278]\n",
      "gradient: [ -3.39892009e+286  -8.45492047e+283]\n",
      "cost: inf\n",
      "loss: [  1.27544793e+285   1.27863653e+285   1.28182513e+285   1.28501373e+285\n",
      "   1.28820233e+285]\n",
      "theta: [  3.18859999e+282   7.93174261e+279]\n",
      "gradient: [  5.15300078e+287   1.28182513e+285]\n",
      "cost: inf\n",
      "loss: [ -1.93366834e+286  -1.93850248e+286  -1.94333662e+286  -1.94817076e+286\n",
      "  -1.95300490e+286]\n",
      "theta: [ -4.83414079e+283  -1.20250770e+281]\n",
      "gradient: [ -7.81230990e+288  -1.94333662e+286]\n",
      "cost: inf\n",
      "loss: [  2.93157656e+287   2.93890546e+287   2.94623435e+287   2.95356325e+287\n",
      "   2.96089214e+287]\n",
      "theta: [  7.32889582e+284   1.82308585e+282]\n",
      "gradient: [  1.18440087e+290   2.94623435e+287]\n",
      "cost: inf\n",
      "loss: [ -4.44447527e+288  -4.45558639e+288  -4.46669751e+288  -4.47780863e+288\n",
      "  -4.48891975e+288]\n",
      "theta: [ -1.11111191e+286  -2.76392577e+283]\n",
      "gradient: [ -1.79563462e+291  -4.46669751e+288]\n",
      "cost: inf\n",
      "loss: [  6.73813563e+289   6.75498086e+289   6.77182610e+289   6.78867133e+289\n",
      "   6.80551656e+289]\n",
      "theta: [  1.68452343e+287   4.19030494e+284]\n",
      "gradient: [  2.72230778e+292   6.77182610e+289]\n",
      "cost: inf\n",
      "loss: [ -1.02154853e+291  -1.02410238e+291  -1.02665624e+291  -1.02921009e+291\n",
      "  -1.03176395e+291]\n",
      "theta: [ -2.55385544e+288  -6.35279560e+285]\n",
      "gradient: [ -4.12720916e+293  -1.02665624e+291]\n",
      "cost: inf\n",
      "loss: [  1.54873908e+292   1.55261090e+292   1.55648272e+292   1.56035455e+292\n",
      "   1.56422637e+292]\n",
      "theta: [  3.87182361e+289   9.63128283e+286]\n",
      "gradient: [  6.25713799e+294   1.55648272e+292]\n",
      "cost: inf\n",
      "loss: [ -2.34799685e+293  -2.35386681e+293  -2.35973676e+293  -2.36560672e+293\n",
      "  -2.37147667e+293]\n",
      "theta: [ -5.86995563e+290  -1.46016990e+288]\n",
      "gradient: [ -9.48625919e+295  -2.35973676e+293]\n",
      "cost: inf\n",
      "loss: [  3.55972759e+294   3.56862685e+294   3.57752611e+294   3.58642538e+294\n",
      "   3.59532464e+294]\n",
      "theta: [  8.89926362e+291   2.21371977e+289]\n",
      "gradient: [  1.43818330e+297   3.57752611e+294]\n",
      "cost: inf\n",
      "loss: [ -5.39679620e+295  -5.41028811e+295  -5.42378002e+295  -5.43727192e+295\n",
      "  -5.45076383e+295]\n",
      "theta: [ -1.34919066e+293  -3.35615414e+290]\n",
      "gradient: [ -2.18038655e+298  -5.42378002e+295]\n",
      "cost: inf\n",
      "loss: [  8.18192082e+296   8.20237549e+296   8.22283017e+296   8.24328484e+296\n",
      "   8.26373952e+296]\n",
      "theta: [  2.04546748e+294   5.08816460e+291]\n",
      "gradient: [  3.30561864e+299   8.22283017e+296]\n",
      "cost: inf\n",
      "loss: [ -1.24043647e+298  -1.24353754e+298  -1.24663861e+298  -1.24973968e+298\n",
      "  -1.25284076e+298]\n",
      "theta: [ -3.10107189e+295  -7.71401371e+292]\n",
      "gradient: [ -5.01154925e+300  -1.24663861e+298]\n",
      "cost: inf\n",
      "loss: [  1.88058852e+299   1.88528996e+299   1.88999140e+299   1.89469284e+299\n",
      "   1.89939429e+299]\n",
      "theta: [  4.70144206e+296   1.16949848e+294]\n",
      "gradient: [  7.59785946e+301   1.88999140e+299]\n",
      "cost: inf\n",
      "loss: [ -2.85110383e+300  -2.85823155e+300  -2.86535926e+300  -2.87248698e+300\n",
      "  -2.87961470e+300]\n",
      "theta: [ -7.12771526e+297  -1.77304155e+295]\n",
      "gradient: [ -1.15188868e+303  -2.86535926e+300]\n",
      "cost: inf\n",
      "loss: [  4.32247299e+301   4.33327910e+301   4.34408522e+301   4.35489134e+301\n",
      "   4.36569745e+301]\n",
      "theta: [  1.08061153e+299   2.68805511e+296]\n",
      "gradient: [  1.74634387e+304   4.34408522e+301]\n",
      "cost: inf\n",
      "loss: [ -6.55317162e+302  -6.56955445e+302  -6.58593728e+302  -6.60232011e+302\n",
      "  -6.61870293e+302]\n",
      "theta: [ -1.63828272e+300  -4.07527971e+297]\n",
      "gradient: [ -2.64757955e+305  -6.58593728e+302]\n",
      "cost: inf\n",
      "loss: [  9.93506690e+303   9.95990442e+303   9.98474193e+303   1.00095794e+304\n",
      "   1.00344170e+304]\n",
      "theta: [  2.48375128e+301   6.17840931e+298]\n",
      "gradient: [  4.01391593e+306   9.98474193e+303]\n",
      "cost: inf\n",
      "loss: [ -1.50622569e+305  -1.50999123e+305  -1.51375677e+305  -1.51752231e+305\n",
      "  -1.52128785e+305]\n",
      "theta: [ -3.76554080e+302  -9.36690100e+299]\n",
      "gradient: [             -inf  -1.51375677e+305]\n",
      "cost: inf\n",
      "loss: [ inf  inf  inf  inf  inf]\n",
      "theta: [              inf   1.42008776e+301]\n",
      "gradient: [ inf  inf]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan -inf]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n",
      "loss: [ nan  nan  nan  nan  nan]\n",
      "theta: [ nan  nan]\n",
      "gradient: [ nan  nan]\n",
      "cost: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mimi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:15: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    }
   ],
   "source": [
    "iterations = 1500\n",
    "alpha = 0.0001\n",
    "(t, c) = gradient_descent(X,y,theta,alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ nan  nan]\n"
     ]
    }
   ],
   "source": [
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "## Prediction\n",
    "print np.array([3.5, 1]).dot(t)\n",
    "print np.array([7, 1]).dot(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Plotting the best fit line\n",
    "best_fit_x = np.linspace(0, 25, 20)\n",
    "best_fit_y = [t[1] + t[0]*xx for xx in best_fit_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e57df90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(X_df.x, y_df, '.')\n",
    "plt.plot(best_fit_x, best_fit_y, '-')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.title('Profit vs. Population with Linear Regression Line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOhbPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e83b750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "[iterations, cost] = zip(*c)\n",
    "plt.plot(iterations, cost, '-')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "cost[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
